@book{Vapnik:1995:NSL:211359,
 author = {Vapnik, Vladimir N.},
 title = {The Nature of Statistical Learning Theory},
 year = {1995},
 isbn = {0-387-94559-8},
 publisher = {Springer-Verlag New York, Inc.},
 address = {New York, NY, USA},
} 

@article{Smola:2004:TSV:1011935.1011939,
 author = {Smola, Alex J. and Sch\"{o}lkopf, Bernhard},
 title = {A Tutorial on Support Vector Regression},
 journal = {Statistics and Computing},
 issue_date = {August 2004},
 volume = {14},
 number = {3},
 month = aug,
 year = {2004},
 issn = {0960-3174},
 pages = {199--222},
 numpages = {24},
 url = {http://dx.doi.org/10.1023/B:STCO.0000035301.49549.88},
 doi = {10.1023/B:STCO.0000035301.49549.88},
 acmid = {1011939},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {machine learning, regression estimation, support vector machines},
} 

@article{Cortes:1995:SN:218919.218929,
 author = {Cortes, Corinna and Vapnik, Vladimir},
 title = {Support-Vector Networks},
 journal = {Mach. Learn.},
 issue_date = {Sept. 1995},
 volume = {20},
 number = {3},
 month = sep,
 year = {1995},
 issn = {0885-6125},
 pages = {273--297},
 numpages = {25},
 doi = {10.1023/A:1022627411411},
 acmid = {218929},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {efficient learning algorithms, neural networks, pattern recognition, polynomial classifiers, radial basis function classifiers},
} 

@techreport{NBERw13553,
 title = "Understanding Recent Trends in House Prices and Home Ownership",
 author = "Robert J. Shiller",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "13553",
 year = "2007",
 month = "October",
 doi = {10.3386/w13553},
 URL = "http://www.nber.org/papers/w13553",
 abstract = {This paper looks at a broad array of evidence concerning the recent boom in home prices, and considers what this means for future home prices and the economy. It does not appear possible to explain the boom in terms of fundamentals such as rents or construction costs. A psychological theory, that represents the boom as taking place because of a feedback mechanism or social epidemic that encourages a view of housing as an important investment opportunity, fits the evidence better.},
}

@INPROCEEDINGS{Quinlan93combininginstance-based,
    author = {J. R. Quinlan},
    title = {Combining Instance-Based and Model-Based Learning},
    booktitle = {},
    year = {1993},
    pages = {236--243},
    publisher = {Morgan Kaufmann}
}

@MISC{Caplin08machinelearning,
    author = {Andrew Caplin and Sumit Chopra and John Leahy and Yann Lecun and Trivikrmaman Thampy},
    title = {Machine Learning and the Spatial Structure of House Prices and Housing Returns},
    year = {2008}
}

@book{belkuhwel80,
  added-at = {2014-01-09T16:23:34.000+0100},
  address = {New York},
  author = {Belsley, D.A. and Kuh, E. and Welsch, R.E.},
  biburl = {http://www.bibsonomy.org/bibtex/2e0e21e8d6fc6c11bf19c29be9f2e9d1a/roman.minguez},
  interhash = {6fc71a8d993832eaae0469c2573484a0},
  intrahash = {e0e21e8d6fc6c11bf19c29be9f2e9d1a},
  keywords = {imported},
  publisher = {John Wiley},
  timestamp = {2014-01-09T16:23:34.000+0100},
  title = {Regression {D}iagnostics: {I}dentifying {I}nfluential {D}ata and {S}ource of {C}ollinearity},
  year = 1980
}

@article{RePEc:eee:jeeman:v:5:y:1978:i:1:p:81-102,
title = {Hedonic housing prices and the demand for clean air},
author = {Harrison, David and Rubinfeld, Daniel L.},
year = {1978},
journal = {Journal of Environmental Economics and Management},
volume = {5},
number = {1},
pages = {81-102},
url = {http://EconPapers.repec.org/RePEc:eee:jeeman:v:5:y:1978:i:1:p:81-102}
}

@article{grossberg1973ces,
    author = {Grossberg, S.},
    citeulike-article-id = {2795975},
    journal = {Studies in Applied Mathematics},
    keywords = {cn550},
    number = {3},
    pages = {213--257},
    posted-at = {2008-05-13 20:03:10},
    priority = {0},
    title = {{Contour enhancement, short term memory, and constancies in reverberating neural networks}},
    volume = {52},
    year = {1973}
}

@book{Min69,
  added-at = {2013-12-05T10:32:26.000+0100},
  address = {Cambridge, MA},
  author = {Minsky, M. and Papert, S.},
  biburl = {http://www.bibsonomy.org/bibtex/24587aec0472c41d00c38bf3e888304ba/prlz77},
  interhash = {9ad5b73f68093070d73e54312145eca2},
  intrahash = {4587aec0472c41d00c38bf3e888304ba},
  keywords = {critic minsky papert perceptron},
  publisher = {MIT Press},
  timestamp = {2013-12-05T10:32:26.000+0100},
  title = {Perceptrons},
  year = 1969
}

@INPROCEEDINGS{GlorotAISTATS2010, 
  author = {Bengio, Yoshua and Glorot, Xavier}, 
  month = may, title = {Understanding the difficulty of training deep feedforward neural networks}, 
  booktitle = {Proceedings of AISTATS 2010}, 
  volume = {9}, 
  year = {2010}, 
  pages = {249-256}, 
  location = {Chia Laguna Resort, Sardinia, Italy}, 
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@MISC{Ng02ondiscriminative,
    author = {Andrew Y. Ng and Michael I. Jordan},
    title = {On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes},
    year = {2002}
}

@article {lecun-98,
original =    {orig/lecun-98.ps.gz},
author = 	{LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
title = 	{Gradient-Based Learning Applied to Document Recognition},
journal =	{Proceedings of the IEEE},
month =         November,
volume =        {86},
number =        {11},
pages =         {2278-2324},
year =		{1998}
}

@book{hastie2009elements,
  title={The elements of statistical learning},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome and Hastie, T and Friedman, J and Tibshirani, R},
  volume={2},
  number={1},
  year={2009},
  publisher={Springer}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and others},
  volume={1},
  year={2006},
  publisher={springer New York}
}

@book{Breiman2001,
  title={Random Forests},
  author={Leo Breiman},
  volume={1},
  year={2001},
  publisher={Kluwer Academic}
}

@book{Manning2008,
  title={Introduction to Information Retrieval},
  author={Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze},
  volume={1},
  year={2008},
  publisher={Cambridge University Press}
}

@inproceedings{Freund1999boosting,
    abstract = {{Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting\&\#039;s relationship to support-vector machines. Some examples of recent applications of boosting are also described.}},
    author = {Freund, Yoav and Schapire, Robert E.},
    booktitle = {In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence},
    citeulike-article-id = {3111663},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.5846},
    keywords = {boosting},
    number = {14},
    pages = {1401--1406},
    posted-at = {2008-08-12 11:51:34},
    priority = {2},
    title = {{A Short Introduction to Boosting}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.5846},
    volume = {14},
    year = {1999}
}

@article{journals/cacm/Domingos12,
  added-at = {2012-10-21T00:00:00.000+0200},
  author = {Domingos, Pedro},
  biburl = {http://www.bibsonomy.org/bibtex/265910b7e73295cb1345a19446810921f/dblp},
  ee = {http://doi.acm.org/10.1145/2347736.2347755},
  interhash = {28f49d94d3029e886460cde63094e482},
  intrahash = {65910b7e73295cb1345a19446810921f},
  journal = {Commun. ACM},
  keywords = {dblp},
  number = 10,
  pages = {78-87},
  timestamp = {2012-10-21T00:00:00.000+0200},
  title = {A few useful things to know about machine learning.},
  url = {http://dblp.uni-trier.de/db/journals/cacm/cacm55.html#Domingos12},
  volume = 55,
  year = 2012
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@MISC{Kivinen97theperceptron,
    author = {J. Kivinen and M. K. Warmuth and P. Auer},
    title = {The Perceptron algorithm vs. Winnow: linear vs. logarithmic mistake bounds when few input variables are relevant},
    year = {1997}
}

@article{DBLP:journals/corr/abs-1206-4656,
  author    = {Kiri Wagstaff},
  title     = {Machine Learning that Matters},
  journal   = {CoRR},
  year      = {2012},
  volume    = {abs/1206.4656},
  url       = {http://arxiv.org/abs/1206.4656},
  timestamp = {Wed, 15 Oct 2014 03:02:24 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1206-4656},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@techreport{fix1951discriminatory,
  title={Discriminatory analysis-nonparametric discrimination: consistency properties},
  author={Fix, Evelyn and Hodges Jr, Joseph L},
  year={1951},
  institution={DTIC Document}
}

@techreport{SpatialDependence,
        author={Bourassa,Steven C. and Cantoni,Eva and Hoesli,Martin E.},
        year={2007},
        title={Spatial Dependence, Housing Submarkets and House Price Prediction},
        type={330; 332/658},
        note={ID: unige:5737},
        abstract={This paper compares alternative methods of controlling for the spatial dependence of house prices in a mass appraisal context. Explicit modeling of the error structure is characterized as a relatively fluid approach to defining housing submarkets. This approach allows the relevant submarket to vary from house to house and for transactions involving other dwellings in each submarket to have varying impacts depending on distance. We conclude that "for our Auckland, New Zealand, data“ the gains in accuracy from including submarket variables in an ordinary least squares specification are greater than any benefits from using geostatistical or lattice methods. This conclusion is of practical importance, as a hedonic model with submarket dummy variables is substantially easier to implement than spatial statistical methods.},
        keywords={Spatial dependence; Hedonic price models; Geostatistical models; Lattice models; Mass appraisal; Housing submarkets},
        language={eng},
        url={http://archive-ouverte.unige.ch/unige:5737},
}

@ARTICLE{RePEc:jre:issued:v:32:n:2:2010:p:139-160,
title = {Predicting House Prices with Spatial Dependence: A Comparison of Alternative Methods},
author = {Bourassa, Steven C. and Cantoni, Eva and Hoesli, Martin},
year = {2010},
journal = {Journal of Real Estate Research},
volume = {32},
number = {2},
pages = {139-160},
abstract = {This paper compares alternative methods for taking spatial dependence into account in house price prediction. We select hedonic methods that have been reported in the literature to perform relatively well in terms of ex-sample prediction accuracy. Because differences in performance may be due to differences in data, we compare the methods using a single data set. The estimation methods include simple OLS, a two-stage process incorporating nearest neighborsâ€™ residuals in the second stage, geostatistical, and trend surface models. These models take into account submarkets by adding dummy variables or by estimating separate equations for each submarket. Based on data for approximately 13,000 transactions from Louisville, Kentucky, we conclude that a geostatistical model with disaggregated submarket variables performs best.},
url = {http://EconPapers.repec.org/RePEc:jre:issued:v:32:n:2:2010:p:139-160}
}

@MISC{Bennett92robustlinear,
    author = {Kristin P. Bennett and O. L. Mangasarian},
    title = {Robust Linear Programming Discrimination Of Two Linearly Inseparable Sets},
    year = {1992}
}

@inproceedings{Schmidhuber:2012:MDN:2354409.2354694,
 author = {Schmidhuber, Jurgen},
 title = {Multi-column Deep Neural Networks for Image Classification},
 booktitle = {Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 series = {CVPR '12},
 year = {2012},
 isbn = {978-1-4673-1226-4},
 pages = {3642--3649},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2354409.2354694},
 acmid = {2354694},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Training,Error analysis,Neurons,Computer architecture,Benchmark testing,Graphics processing unit},
}

@inproceedings{Oshiro2012,
abstract = {Random Forest is a computationally efficient technique that can operate quickly over large datasets. It has been used in many recent research projects and real-world applications in diverse domains. However, the associated literature provides almost no directions about how many trees should be used to compose a Random Forest. The research reported here analyzes whether there is an optimal number of trees within a Random Forest, i.e., a threshold from which increasing the number of trees would bring no significant performance gain, and would only increase the computational cost. Our main conclusions are: as the number of trees grows, it does not always mean the performance of the forest is significantly better than previous forests (fewer trees), and doubling the number of trees is worthless. It is also possible to state there is a threshold beyond which there is no significant gain, unless a huge computational environment is available. In addition, it was found an experimental relationship for the AUC gain when doubling the number of trees in any forest. Furthermore, as the number of trees grows, the full set of attributes tend to be used within a Random Forest, which may not be interesting in the biomedical domain. Additionally, datasets’ density-based metrics proposed here probably capture some aspects of the VC dimension on decision trees and low-density datasets may require large capacity machines whilst the opposite also seems to be true.},
author = {Oshiro, Thais Mayumi and Perez, Pedro Santoro and Baranauskas, Jos\'{e} Augusto},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-31537-4\_13},
isbn = {9783642315367},
issn = {03029743},
keywords = {Number of Trees,Random Forest,VC Dimension},
pages = {154--168},
title = {{How many trees in a random forest?}},
volume = {7376 LNAI},
year = {2012}
}
@article{pybrain2010jmlr,
	Author = {Schaul, Tom and Bayer, Justin and Wierstra, Daan and Sun, Yi and 
	          Felder, Martin and Sehnke, Frank and R{\"u}ckstie{\ss}, Thomas and Schmidhuber, J{\"u}rgen},
	Journal = {Journal of Machine Learning Research},
	Pages = {743--746},
	Title = {{PyBrain}},
	Volume = {11},
	Year = {2010}
}
@article{Guyon2002,
abstract = {DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these newmicro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues. In this paper, we address the problem of selection of a small subset of genes from broad patterns of gene expression data, recorded on DNA micro-arrays. Using available training examples from cancer and normal patients, we build a classifier suitable for genetic diagnosis, as well as drug discovery. Previous attempts to address this problem select genes with correlation techniques.We propose a newmethod of gene selection utilizing Support Vector Machine methods based on Recursive Feature Elimination (RFE).We demonstrate experimentally that the genes selected by our techniques yield better classification performance and are biologically relevant to cancer. In contrast with the baseline method, our method eliminates gene redundancy automatically and yields better and more compact gene subsets. In patients with leukemia our method discovered 2 genes that yield zero leave- one-out error, while 64 genes are necessary for the baseline method to get the best result (one leave-one-out error). In the colon cancer database, using only 4 genes our method is 98\% accurate, while the baseline method is only 86\% accurate.},
author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
doi = {10.1023/A:1012487302797},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Cancer classification,DNA micro-array,Diagnosis,Diagnostic tests,Drug discovery,Feature selection,Gene selection,Genomics,Proteomics,RNA expression,Recursive feature elimination,Support vector machines},
pages = {389--422},
pmid = {21889629},
title = {{Gene selection for cancer classification using support vector machines}},
volume = {46},
year = {2002}
}
@article{Shiller2007,
abstract = {This paper looks at a broad array of evidence concerning the recent boom in home prices, and considers what this means for future home prices and the economy. It does not appear possible to explain the boom in terms of fundamentals such as rents or construction costs. A psychological theory, that represents the boom as taking place because of a feedback mechanism or social epidemic that encourages a view of housing as an important investment opportunity, fits the evidence better.},
author = {Shiller, Robert J.},
journal = {Housing, Housing Finance and Monetary Policy, Jackson Hole Conference Series, Federal Reserve Bank of Kansas City},
pages = {85--123},
title = {{Understanding Recent Trends in House Prices and Home Ownership}},
year = {2007}
}



	